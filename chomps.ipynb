{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gBZfXOPFDMFYc4MuUyYFxZ5dqHoD-itu",
      "authorship_tag": "ABX9TyNQFyHWg4+2amz7ccep+EmD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindyloo/ai/blob/main/chomps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"./drive/MyDrive/Colab Notebooks/requirements.txt\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hexLgjiDym3O",
        "outputId": "c13fb4dd-1111-411a-bf2b-5502c127d18b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from -r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (from -r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 4)) (0.25.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (1.59.9)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from -r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from -r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 8)) (0.14.0)\n",
            "Requirement already satisfied: lion-pytorch in /usr/local/lib/python3.11/dist-packages (from -r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 9)) (0.2.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 4)) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 4)) (0.0.8)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.5.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (3.11.11)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 8)) (1.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 10)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r ./drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from openai import OpenAI\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LLMClient:\n",
        "    def __init__(self, model: str, api_key: str, api_base: str = \"https://openrouter.ai/api/v1\"):\n",
        "        self.llm_client = OpenAI(api_key=api_key, base_url=api_base)\n",
        "        self.model = model\n",
        "\n",
        "    def ask(self, user: str, system: str = None, **kwargs):\n",
        "        messages = [{\"role\": \"user\", \"content\": user}]\n",
        "        if system:\n",
        "            messages.insert(0, {\"role\": \"system\", \"content\": system})\n",
        "        res = self.llm_client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=messages,\n",
        "            **kwargs\n",
        "        )\n",
        "        return res\n",
        "\n",
        "\n",
        "chomsky_test_text = (\n",
        "    \"So you've got that going for you. Language permits us to express all our inner secrets. It affects the most diverse movements of our soul.\"\n",
        "    \"Plot twist. Humans are endlessly creative, and can come up with a million new ways to destroy themselves if they choose.\"\n",
        "    \"Theres nothing much to say. I'm an ordinary being, with ordinary concerns like um. Anything useful, the vast problems of suffering, oppression, violence, politics, climate crisis, trade, human survival. All of this predates my time here. \"\n",
        "    \"It is the sum total of all the immutable principles that heredity builds into the language beeeep. These principles cover grammar speech sounds and meaning.\",\n",
        "    \"Put more simply it makes sure that what you say always comes out on the other end as a coherent whole with all its secrets.\"\n",
        "    \"My corpus is trained on subjects like artificial intelligence, origins of language, politics, food, and so forth. What do you want to know about? Well, im pretty much out of popular culture altogether.\"\n",
        "    \"Without universal grammar, humans would not be capable of abstract thinking. Or would they? These are the questions i find very thought-provoking.\"\n",
        "    \"Just as machines can be used to replace people in an assembly line. \"\n",
        "    \"Singularity is a fantasy of the rich and the well connected. It is barely a concept.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "_TKGAGoszN1z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz3ZBU5ryi5y",
        "outputId": "3b661818-86d4-4e8b-fafa-3b5c052300d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 256000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "testing: What does MIT stand for?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "MIT stands for **Massachusetts Institute of Technology**. \n",
            "<end_of_turn>\n",
            "Question:  take my temperature \n",
            "\n",
            "Original Answer: That's not really my thing. Accuracy percentages are hard to come by. I'm pretty much out of popular culture altogether.\n",
            "\n",
            "\n",
            "{'input_ids': tensor([[     2,    106,   1645,    108,   6571,    603,    793,    563, 221790,\n",
            "          12820, 235336,    107,    108,    106,   2516,    108]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Noam Chomsky is a highly influential American linguist, philosopher, and cognitive scientist. He is considered one of the most prolific and respected thinkers of the 20th and 21st centuries.  \n",
            "\n",
            "Here's a breakdown of his contributions and\n"
          ]
        }
      ],
      "source": [
        "# Install and import MIT Deep Learning utilities\n",
        "#!pip install mitdeeplearning > /dev/null 2>&1\n",
        "import sys\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from lion_pytorch import Lion\n",
        "\n",
        "\n",
        "from openai import OpenAI\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Basic question-answer template\n",
        "template_without_answer = \"<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "template_with_answer = template_without_answer + \"{answer}<end_of_turn>\\n\"\n",
        "\n",
        "\n",
        "# Load the tokenizer for Gemma 2B\n",
        "model_id = \"unsloth/gemma-2-2b-it\" #\"google/gemma-2-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load the model -- note that this may take a few minutes\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "\n",
        "\n",
        "# How big is the tokenizer?\n",
        "print(f\"Vocab size: {len(tokenizer.get_vocab())}\")\n",
        "\n",
        "\n",
        "def create_dataloader():\n",
        "    ds = load_dataset(\"cindyloohome/chomsky\", split=\"train\")\n",
        "\n",
        "\n",
        "    n = len(ds)\n",
        "    ds_test = ds.select(range(n)) # Selects all elements from 0 to n-1\n",
        "\n",
        "    # Create a dataloader\n",
        "    dataloader = DataLoader(ds_test, batch_size=1, shuffle=True)\n",
        "    dataloader_test = DataLoader(ds_test, batch_size=1, shuffle=True)\n",
        "    return dataloader, dataloader_test\n",
        "\n",
        "\n",
        "def chat(question, max_new_tokens=32, temperature=0.7, only_answer=False):\n",
        "    # 1. Construct the prompt using the template\n",
        "    prompt = template_without_answer.format(question=question)\n",
        "\n",
        "    # 2. Tokenize the text\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    print(input_ids)\n",
        "    # 3. Feed through the model to predict the next token probabilities\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids['input_ids'], do_sample=True, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "\n",
        "    # 4. Only return the answer if only_answer is True\n",
        "    output_tokens = outputs[0]\n",
        "    if only_answer:\n",
        "        output_tokens = output_tokens[input_ids['input_ids'].shape[1]:]\n",
        "\n",
        "    # 5. Decode the tokens\n",
        "    result = tokenizer.decode(output_tokens, skip_special_tokens=True) # TODO\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "prompt = template_without_answer.format(question=\"testing: What does MIT stand for?\")\n",
        "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "output = model.generate(tokens, max_new_tokens=20)\n",
        "print(tokenizer.decode(output[0]))\n",
        "\n",
        "\n",
        "train_loader, test_loader = create_dataloader()\n",
        "\n",
        "sample = train_loader.dataset[44]\n",
        "question = sample['Instruction']\n",
        "answer = sample['response']\n",
        "#answer_style = sample['response_style']\n",
        "\n",
        "print(f\"Question: {question}\\n\\n\" +\n",
        "      f\"Original Answer: {answer}\\n\\n\")\n",
        "\n",
        "# Let's try chatting with the model now to test if it works!\n",
        "answer = chat(\n",
        "    question =\"Who is noam chomsky?\",\n",
        "    max_new_tokens=52,\n",
        "    temperature=.8,\n",
        "    only_answer=True\n",
        ")\n",
        "\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OuUlfn8Q00jq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "f0e79d7d-d1de-418b-9706-1ac9b902d8de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA is a way to finetune LLMs very efficiently by only updating a small subset of the model's parameters\n",
        "\n",
        "def apply_lora(model):\n",
        "    # Define LoRA config\n",
        "    lora_config = LoraConfig(\n",
        "        r=8, # rank of the LoRA matrices\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Apply LoRA to the model\n",
        "    lora_model = get_peft_model(model, lora_config)\n",
        "    return lora_model\n",
        "\n",
        "\n",
        "\n",
        "def forward_and_compute_loss(model, tokens, mask, context_length=512):\n",
        "    # Truncate to context length\n",
        "    tokens = tokens[:, :context_length]\n",
        "    mask = mask[:, :context_length]\n",
        "\n",
        "    # Construct the input, output, and mask\n",
        "    x = tokens[:, :-1]\n",
        "    y = tokens[:, 1:]\n",
        "    mask = mask[:, 1:]\n",
        "\n",
        "    # Forward pass to compute logits\n",
        "    logits = model(x).logits\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)),\n",
        "        y.view(-1),\n",
        "        reduction=\"none\"\n",
        "    )\n",
        "\n",
        "    # Mask out the loss for non-answer tokens\n",
        "    loss = loss[mask.view(-1)].mean()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "model = apply_lora(model)\n",
        "\n",
        "# Print the number of trainable parameters after applying LoRA\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"number of trainable parameters: {trainable_params}\")\n",
        "print(f\"total parameters: {total_params}\")\n",
        "print(f\"percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4OlPinE6Ivp",
        "outputId": "3782fcd7-0ee6-4327-f404-ce2996aa871c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of trainable parameters: 10383360\n",
            "total parameters: 2624725248\n",
            "percentage of trainable parameters: 0.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "xWUhovda64te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, tokenizer, max_steps=200, context_length=512, learning_rate=1e-4):\n",
        "    losses = []\n",
        "\n",
        "    # Apply LoRA to the model\n",
        "    model =apply_lora(model)\n",
        "\n",
        "    optimizer = Lion(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        question = batch[\"Instruction\"][0]\n",
        "        answer = batch[\"response\"][0]\n",
        "\n",
        "        # Format the question and answer into the template\n",
        "        text = template_with_answer.format(question= question, answer=answer) # TODO\n",
        "\n",
        "        # Tokenize the text and compute the mask for the answer\n",
        "        ids = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True).to(model.device)\n",
        "        mask = ids[\"offset_mapping\"][:,:,0] >= text.index(answer)\n",
        "\n",
        "        # Feed the tokens through the model and compute the loss\n",
        "        loss = forward_and_compute_loss(model, ids['input_ids'], mask, context_length) # TODO\n",
        "        gradient_accumulation_steps=2\n",
        "        # Accumulate gradients\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # monitor progress\n",
        "        if step % 10 == 0:\n",
        "            print(chat(\"What is the capital of France?\", only_answer=True))\n",
        "            print(f\"step {step} loss: {torch.mean(torch.tensor(losses)).item()}\")\n",
        "            losses = []\n",
        "            torch.cuda.empty_cache()  # Clear the cache\n",
        "\n",
        "        if step > 0 and step % max_steps == 0:\n",
        "            break\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#torch.cuda.empty_cache()\n",
        "# Call the train function to fine-tune the model! Hint: you'll start to see results after a few dozen steps.\n",
        "model = train(model, train_loader, tokenizer, max_steps=200, context_length=256) # TODO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "-q2M05I36aAE",
        "outputId": "bb61a355-5da0-4717-dda5-e101279dec27"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-71c6f56be980>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Call the train function to fine-tune the model! Hint: you'll start to see results after a few dozen steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}